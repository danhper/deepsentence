#!/usr/bin/env python

import logging

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy.utils.log import configure_logging

from sqlalchemy.orm import joinedload

from twisted.internet import defer

from deep_sentence import db, models
from deep_sentence.utils import pid_utils


NAME = 'fetch_sources'
BATCH_SIZE = 300


def clean_exit():
    pid_utils.clean_pid_file(NAME)


def create_metadata(sources):
    metadata = {}
    for source in sources:
        metadata[source.url] = {'id': source.id, 'base_url': source.media.base_url}
    return metadata


def query_sources(session, offset=0, count=False, limit=BATCH_SIZE):
    base_query = session.query(models.Source). \
                         join(models.Source.media). \
                         options(joinedload(models.Source.media)). \
                         filter(models.Source.content == None). \
                         offset(offset)
    if count:
        return base_query.count()
    else:
        return base_query.limit(limit).all()


@defer.inlineCallbacks
def fetch_batch(runner, session_maker, offset=0):
    with db.session_scope(session_maker) as session:
        original_count = query_sources(session, count=True)
        sources = query_sources(session, offset=offset)
        if not sources:
            return
        urls_metadata = create_metadata(sources)

    yield runner.crawl('source_content', urls_metadata=urls_metadata)

    with db.session_scope(session_maker) as session:
        new_count = query_sources(session, count=True)

    logging.info('batch finished. articles without content: original=%d, new=%d',
                 original_count, new_count)
    if new_count < original_count:
        offset_delta = BATCH_SIZE - (original_count - new_count)
        fetch_batch(runner, session_maker, offset=offset + offset_delta)
    else:
        fetch_batch(runner, session_maker, offset=offset + BATCH_SIZE)


def main():
    pid_utils.check_and_write_pid(NAME)
    configure_logging()
    runner = CrawlerProcess(get_project_settings())
    session_maker = db.create_session_maker()
    fetch_batch(runner, session_maker)
    runner.start()
    clean_exit()


if __name__ == '__main__':
    main()
