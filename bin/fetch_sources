#!/usr/bin/env python

import logging

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from scrapy.utils.log import configure_logging

from sqlalchemy.orm import joinedload

from twisted.internet import defer

from deep_sentence.scraper import source_parsers
from deep_sentence import db, models
from deep_sentence.utils import pid_utils


NAME = 'fetch_sources'
BULK_SIZE = 300
PARSABLE_URLS = [url for parser in source_parsers.get_parsers() for url in parser.supported_urls]


def clean_exit():
    pid_utils.clean_pid_file(NAME)


def create_metadata(sources):
    metadata = {}
    for source in sources:
        metadata[source.url] = {'id': source.id, 'base_url': source.media.base_url}
    return metadata


def query_sources(session, count=False, limit=BULK_SIZE):
    base_query = session.query(models.Source). \
                         join(models.Source.media). \
                         options(joinedload(models.Source.media)). \
                         filter(models.Source.content == None). \
                         filter(models.Media.base_url.in_(PARSABLE_URLS))
    if count:
        return base_query.count()
    else:
        return base_query.limit(limit).all()


@defer.inlineCallbacks
def fetch_batch(runner, session_maker):
    with db.session_scope(session_maker) as session:
        original_count = query_sources(session, count=True)
        if original_count == 0:
            clean_exit()
        sources = query_sources(session)
        urls_metadata = create_metadata(sources)

    yield runner.crawl('source_content', urls_metadata=urls_metadata)

    with db.session_scope(session_maker) as session:
        new_count = query_sources(session, count=True)

    logging.info('batch finished. articles without content: original=%d, new=%d',
                original_count, new_count)
    if new_count < original_count:
        fetch_batch(runner, session_maker)
    else:
        clean_exit()


def main():
    pid_utils.check_and_write_pid(NAME)
    configure_logging()
    runner = CrawlerProcess(get_project_settings())
    session_maker = db.create_session_maker()
    fetch_batch(runner, session_maker)
    runner.start()


if __name__ == '__main__':
    main()
